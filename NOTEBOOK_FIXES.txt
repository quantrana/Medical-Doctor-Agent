================================================================================
                        COLAB NOTEBOOK FIXES
================================================================================

CRITICAL BUGS FOUND:
--------------------
1. git pull runs BEFORE cd to correct directory
2. TRL version mismatch (using 0.21.0, should be >=0.14.0)
3. Missing --num_machines and --machine_rank flags
4. Missing --deepspeed_multinode_launcher flag
5. Wrong GRPO arguments (num_generations, max_completion_length, temperature)
6. Missing required training arguments

CORRECTED NOTEBOOK:
-------------------

from google.colab import drive
drive.mount('/content/drive')

# FIXED: CD FIRST, THEN PULL
%cd /content/drive/MyDrive/43008/Assignment 3/Medical-Doctor-Agent/
!git pull origin main

# FIXED: TRL VERSION
!uv pip install deepspeed vllm
!uv pip install trl>=0.14.0
!uv pip install flash-attn --no-build-isolation
!uv pip install "sglang[all]>=0.5.3rc0"

# SFT Training (unchanged)
!accelerate launch --config_file ./configs/deepspeed_zero3.yaml \
    --num_processes 1 \
    --num_machines 1 \
    --machine_rank 0 \
    --deepspeed_multinode_launcher standard ./scripts/train_sft.py \
    --model_path models/Qwen2.5-1.5B-Instruct \
    --data_path data/demo_data.json \
    --train_bsz_per_gpu 2 \
    --output_dir ./ckpts

# PPO Training (unchanged)
!accelerate launch \
    --num_processes 1 \
    --num_machines 1 \
    --machine_rank 0 \
    --config_file ./configs/deepspeed_zero3.yaml \
    --deepspeed_multinode_launcher standard ./scripts/train_rl.py \
    --model_name_or_path ./ckpts/sft_stage1/checkpoint-0-99/tfmr \
    --reward_model_path models/medical_o1_verifier_3B \
    --value_model_path Qwen/Qwen3-0.6B \
    --dataset_name data/medical_o1_verifiable_problem.json \
    --response_length 1300 \
    --temperature 0.5 \
    --local_rollout_forward_batch_size 8 \
    --num_ppo_epochs 3 \
    --num_mini_batches 1 \
    --total_episodes 1024 \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 16 \
    --bf16 True \
    --output_dir ./ckpts \
    --save_strategy steps \
    --save_step 16 \
    --save_total_limit 1 \
    --eval_strategy steps \
    --eval_steps 16 \
    --kl_coef 0.03 \
    --learning_rate 5e-7 \
    --warmup_ratio 0.05 \
    --gradient_checkpointing True \
    --dataloader_num_workers 4 \
    --run_name ppo_medical_o1 \
    --num_sample_generations -1

# GRPO Training (FIXED)
!accelerate launch \
    --num_processes 1 \
    --num_machines 1 \
    --machine_rank 0 \
    --config_file ./configs/deepspeed_zero3.yaml \
    --deepspeed_multinode_launcher standard \
    scripts/train_grpo.py \
    --model_name_or_path ./ckpts/sft_stage1/checkpoint-0-99/tfmr \
    --reward_model_path models/medical_o1_verifier_3B \
    --dataset_name data/medical_o1_verifiable_problem.json \
    --output_dir ./ckpts/grpo_medical_o1 \
    --run_name grpo_medical_o1 \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 16 \
    --learning_rate 5e-7 \
    --num_train_epochs 1 \
    --bf16 True \
    --logging_steps 10 \
    --save_strategy steps \
    --save_steps 100 \
    --eval_strategy steps \
    --eval_steps 100 \
    --gradient_checkpointing True \
    --warmup_ratio 0.05

================================================================================
                          KEY CHANGES
================================================================================

DIRECTORY ORDER:
  Before: !git pull → %cd
  After:  %cd → !git pull

TRL VERSION:
  Before: trl==0.21.0
  After:  trl>=0.14.0

ACCELERATE FLAGS (GRPO):
  Added: --num_machines 1
  Added: --machine_rank 0  
  Added: --deepspeed_multinode_launcher standard

GRPO ARGUMENTS:
  Removed: --num_generations 4
  Removed: --max_completion_length 1300
  Removed: --temperature 0.5
  Added:   --num_train_epochs 1
  Added:   --bf16 True (not just --bf16)
  Added:   --logging_steps 10
  Added:   --save_strategy steps
  Added:   --save_steps 100
  Added:   --eval_strategy steps
  Added:   --eval_steps 100
  Added:   --gradient_checkpointing True (not just --gradient_checkpointing)
  Added:   --warmup_ratio 0.05

================================================================================
                        EXPECTED OUTPUT
================================================================================

When you run the corrected GRPO command, you should see:

[FIXED] Embedding padding_idx: XXXXX -> 128000
Reward tokenizer pad_token_id: 128000
Reward model vocab size: 128256
Reward model config pad_token_id: 128000

Then training will start without the "Padding_idx must be within num_embeddings" error.

================================================================================

